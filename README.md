<p align="center">
  <img src="docs/logo.png" alt="Yap" width="120" />
</p>

<h1 align="center">Yap</h1>

<p align="center">
  <strong>The voice input layer for agentic coding.</strong><br/>
  Speak in any language. It transcribes, corrects, translates, and types â€” right where your cursor is.
</p>

<p align="center">
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-CC%20BY--NC%204.0-blue.svg" alt="License" /></a>
  <img src="https://img.shields.io/badge/platform-macOS%20(Apple%20Silicon)-black?logo=apple" alt="Platform" />
  <img src="https://img.shields.io/badge/runtime-100%25%20local-brightgreen" alt="Local" />
</p>

---

## ðŸŽ¬ Demo

> ðŸŽ¥ Demo video coming soon â€” stay tuned!

> ðŸ’¡ **Inspired by the agentic coding movement** â€” like [OpenClaw](https://github.com/openclaw/openclaw)'s founder voice-chatting with 10+ agents to build software. Yap is the missing input layer that makes talking to your dev tools feel native.

<!-- Optional: embed a video showing Yap + Claude Code / Cursor workflow -->
<!-- https://github.com/user-attachments/assets/agentic-workflow.mp4 -->

---

## ðŸ¤” Why Yap?

The agentic coding era is here. You're talking to Claude Code, Cursor, Copilot â€” but you're still *typing* every prompt with your fingers.

**Your voice is 3x faster than your keyboard.** Yap bridges the gap.

- ðŸ—£ï¸ **Voice-first workflow** â€” Talk to your agents, your terminal, your browser. Yap types it out.
- ðŸ”’ **100% local** â€” On-device VAD + ASR via MLX. No cloud. No data leaves your machine.
- ðŸŒ **Multilingual** â€” Speak Chinese, English, Japanese, Korean, and more. Real-time translation built in.
- âœ¨ **Smart correction** â€” LLM-powered spoken â†’ written style conversion. Your voice, but polished.

---

## âš¡ How It Works

Yap lives as a floating ball on your screen. Toggle input mode, and it listens:

```
ðŸŽ™ï¸ Voice â”€â”€â†’ ðŸ”‡ VAD â”€â”€â†’ ðŸ§  ASR â”€â”€â†’ ðŸ’¬ LLM â”€â”€â†’ âŒ¨ï¸ Input
             Silero      MLX       Correct    Types into
             detects     on-device  & Translate active app
             speech      transcribe (optional)
```

Models auto-download from HuggingFace on first launch. Zero config to get started.

---

## âœ¨ Features

| | Feature | Description |
|---|---------|-------------|
| ðŸŽ™ï¸ | **Multilingual Voice Input** | Chinese, English, Japanese, and more â€” switch on the fly |
| ðŸŒ | **Real-time Translation** | Speak in one language, type in another |
| âœï¸ | **Formal Correction** | Spoken â†’ written style, powered by any LLM |
| ðŸ–¥ï¸ | **Universal Input** | Works with any app â€” Claude Code, Cursor, VS Code, Terminal, browser, Slack... |
| ðŸ«§ | **Floating Ball UI** | Always-on-top, draggable, with live waveform visualization |
| ðŸ”’ | **Fully Local** | On-device ASR, no cloud dependency, your data stays yours |
| ðŸŒ | **i18n Menu** | ä¸­æ–‡ / English interface |

---

## ðŸš€ Quick Start

### Prerequisites

- macOS with Apple Silicon (M1/M2/M3/M4)
- Node.js 18+
- Rust (latest stable)
- Python 3.10 â€“ 3.12
- [uv](https://github.com/astral-sh/uv)

### Install & Run

```bash
# Clone
git clone https://github.com/TorchFun-AI/Yap.git && cd Yap

# Install frontend dependencies
cd src-frontend && npm install && cd ..

# Install Python dependencies
cd src-backend && uv sync && cd ..

# Terminal 1 â€” Start Python AI backend
cd src-backend && uv run python main.py

# Terminal 2 â€” Start Tauri dev server
cd src-tauri && npm run tauri dev
```

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vue 3 UI      â”‚â—„â”€â”€â”€â–ºâ”‚   Tauri Core    â”‚â—„â”€â”€â”€â–ºâ”‚  Python AI      â”‚
â”‚   (Webview)     â”‚ IPC â”‚   (Rust)        â”‚ WS  â”‚  (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                        â”‚
                              â–¼                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Keyboard  â”‚           â”‚ VAD + ASR â”‚
                        â”‚ Simulationâ”‚           â”‚   + LLM   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Layer | Stack |
|-------|-------|
| Frontend | Vue 3 + TypeScript + Ant Design Vue + Pinia |
| Core | Tauri 2 (Rust) |
| Backend | Python + FastAPI + Silero VAD + MLX Audio |

---

## ðŸ”§ LLM Configuration

Yap uses any **OpenAI-compatible API** for text correction and translation. Configure in Settings:

- API Key
- Base URL (e.g. `https://api.openai.com/v1`, or a local Ollama endpoint)
- Model name

> This is optional â€” without it, Yap still does voice-to-text perfectly fine.

---

## ðŸ“„ License

[CC BY-NC 4.0](LICENSE) â€” Free to use, modify, and share. Not for commercial use.
